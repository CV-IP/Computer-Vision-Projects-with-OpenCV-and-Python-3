{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml easydict munkres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "from imageio import imread\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from config import load_config\n",
    "from nnet.net factory import pose_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_pose_prediction(cfg):\n",
    "    inputs = tf.placeholder(tf.float32, shape=[cfg.batch_size, None, None, 3])\n",
    "\n",
    "    outputs = pose_net(cfg).test(inputs)\n",
    "\n",
    "    restorer = tf.train.Saver()\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    # Restore variables from disk.\n",
    "    restorer.restore(sess, cfg.init_weights)\n",
    "\n",
    "    return sess, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cnn_output(outputs_np, cfg, pairwise_stats = None):\n",
    "    scmap = outputs_np['part_prob']\n",
    "    scmap = np.squeeze(scmap)\n",
    "    locref = None\n",
    "    pairwise_diff = None\n",
    "    if cfg.location_refinement:\n",
    "        locref = np.squeeze(outputs_np['locref'])\n",
    "        shape = locref.shape\n",
    "        locref = np.reshape(locref, (shape[0], shape[1], -1, 2))\n",
    "        locref *= cfg.locref_stdev\n",
    "    if cfg.pairwise_predict:\n",
    "        pairwise_diff = np.squeeze(outputs_np['pairwise_pred'])\n",
    "        shape = pairwise_diff.shape\n",
    "        pairwise_diff = np.reshape(pairwise_diff, (shape[0], shape[1], -1, 2))\n",
    "        num_joints = cfg.num_joints\n",
    "        for pair in pairwise_stats:\n",
    "            pair_id = (num_joints - 1) * pair[0] + pair[1] - int(pair[0] < pair[1])\n",
    "            pairwise_diff[:, :, pair_id, 0] *= pairwise_stats[pair][\"std\"][0]\n",
    "            pairwise_diff[:, :, pair_id, 0] += pairwise_stats[pair][\"mean\"][0]\n",
    "            pairwise_diff[:, :, pair_id, 1] *= pairwise_stats[pair][\"std\"][1]\n",
    "            pairwise_diff[:, :, pair_id, 1] += pairwise_stats[pair][\"mean\"][1]\n",
    "    return scmap, locref, pairwise_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_pose_predict(scmap, offmat, stride):\n",
    "    \"\"\"Combine scoremat and offsets to the final pose.\"\"\"\n",
    "    num_joints = scmap.shape[2]\n",
    "    pose = []\n",
    "    for joint_idx in range(num_joints):\n",
    "        maxloc = np.unravel_index(np.argmax(scmap[:, :, joint_idx]),\n",
    "                                  scmap[:, :, joint_idx].shape)\n",
    "        offset = np.array(offmat[maxloc][joint_idx])[::-1] if offmat is not None else 0\n",
    "        pos_f8 = (np.array(maxloc).astype('float') * stride + 0.5 * stride +\n",
    "                  offset)\n",
    "        pose.append(np.hstack((pos_f8[::-1],\n",
    "                               [scmap[maxloc][joint_idx]])))\n",
    "    return np.array(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"demo/pose_cfg.yaml\")\n",
    "sess, inputs, outputs = setup_pose_prediction(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"testcases/standing-lef-lift.jpg\"\n",
    "image = np.array(imread(file_name))\n",
    "image_batch = np.expand_dims(image, axis=0).astype(float)\n",
    "outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})\n",
    "scmap, locref, pairwise_diff = extract_cnn_output(outputs_np, cfg)\n",
    "pose = argmax_pose_predict(scmap, locref, cfg.stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose2D = pose[:, :2]\n",
    "image_annot = image.copy()\n",
    "\n",
    "for index in range(5):\n",
    "    randcolor = tuple([randint(0, 255) for i in range(3)])\n",
    "    thickness = int(min(image_annot[:,:,0].shape)/250) + 1\n",
    "    start_pt = tuple(pose2D[index].astype('int'))\n",
    "    end_pt = tuple(pose2D[index+1].astype('int'))\n",
    "    image_annot = cv2.line(image_annot, start_pt, end_pt, randcolor, thickness)\n",
    "for index in range(6,11): #next bunch are arms/shoulders (from one hand to other)\n",
    "    randcolor = tuple([randint(0,255) for i in range(3)])\n",
    "    thickness = int(min(image_annot[:,:,0].shape)/250) + 1\n",
    "    start_pt = tuple(pose2D[index].astype('int'))\n",
    "    end_pt = tuple(pose2D[index+1].astype('int'))\n",
    "    image_annot = cv2.line(image_annot, start_pt, end_pt, randcolor, thickness)\n",
    "#connect Line from chin to top of head\n",
    "image_annot = cv2.line(image_annot,\n",
    "                       tuple(pose2D[12].astype('int')), tuple(pose2D[13].astype('i\n",
    "                       tuple([randint(0,255) for i in range(3)]), thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There no actual joints on waist or coLLar,\n",
    "# but we can estimate them from hip/shoulder midpoints\n",
    "waist = tuple(((pose2D[2]+pose2D[3])/2).astype('int'))\n",
    "collar = tuple(((pose2D[8]+pose2D[9])/2).astype('int'))\n",
    "# draw the \"spine\"\n",
    "image_annot = cv2.line(image_annot, waist, collar,\n",
    "                       tuple([randint(0,255) for i in range(3)]), thickness)\n",
    "image_annot = cv2.line(image_annot, tuple(pose2D[12].astype('int')), collar,\n",
    "                       tuple([randint(0,255) for i in range(3)]), thickness)\n",
    "# now Label the joints with numbers\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontsize = min(image_annot[:,:,0].shape)/750 #scale the font size to the image size\n",
    "for idx, pt in enumerate(pose2D):\n",
    "    randcolor = tuple([randint(0,255) for i in range(3)])\n",
    "image_annot = cv2.putText(image_annot, str(idx+1),\n",
    "                          tup1e(pt.astype('int')),font, fontsize,\n",
    "                          randcolor,2,cv2.LINE_AA)\n",
    "figure()\n",
    "imshow(image_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2 I\n",
    "from imageio import imread, imsave\n",
    "from config import load_config\n",
    "from dataset.factory import create as create_dataset\n",
    "from nnet import predict\n",
    "from dataset.pose_dataset import data_to_input\n",
    "from multiperson.detections import extract_detections\n",
    "from multiperson.predict import SpatialModel, eval_graph, get_person_conf_mu1ticut\n",
    "# from muLtiperson.visuaLize import PersonDraw, visuaLize_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = load_config(\"demo/pose_cfg_multi.yaml) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(cfg)\n",
    "sm = SpatialModel(cfg)\n",
    "sm.load()\n",
    "sess, inputs, outputs = predict.setup_pose_prediction(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"testcases/bus_people.jpg\"\n",
    "image = np.array(imread(file_name))\n",
    "image_batch = data_to_input(image)\n",
    "# Compute prediction with the CNN\n",
    "outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})\n",
    "scmap, locref, pairwise_diff = predict.extract_cnn_output(outputs_np, cfg, dataset\n",
    "detections = extract_detections(cfg, scmap, locref, pairwise_diff)\n",
    "unLab, pos_array, unary_array, pwidx_array, pw_array = eval_graph(sm, detections)\n",
    "person_conf_multi = get_person_conf_multicut(sm, unLab, unary_array, pos_array)\n",
    "image_annot = image.copy()\n",
    "for pose2D in person_conf_mu1ti:\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontsize = min(image_annot[:,:,0].shape)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_path = \"./shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_fronta1_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "#Uncomment Line below if you want to use your webcam\n",
    "#cap = cv2.VideoCapture(0) #0 is the first camera on your computer, change if you\n",
    "#more than one camera\n",
    "\n",
    "#Comment out the Line below if using webcam\n",
    "cap = cv2.VideoCapture('./rollerc.mp4')\n",
    "figure(100)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "while(True):\n",
    "    #Capture frame-by-frame\n",
    "    ret, img = cap.read()\n",
    "    img.flags['WRITEABLE']=True #just in case\n",
    "  \n",
    "    try:\n",
    "        dets = detector(img, 1)\n",
    "        shape = predictor(img, dets[0])\n",
    "    except:\n",
    "        print('no face detected', end='\\r')\n",
    "        cap.release()\n",
    "        break\n",
    "#similar to previous example, except frame-by-frame here\n",
    "    annotated=img.copy()\n",
    "    head_width = shape.part(16).x-shape.part(6).x\n",
    "    fontsize = head_width/650\n",
    "    for pt in range(68):\n",
    "        x,y = shape.part(pt).x, shape.part(pt).y\n",
    "        annotated=cv2.putText(annotated, str(pt), (x,y), font, fontsize, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "#Let’s see our resuLts\n",
    "    fig=imshow(cv2.cvtColor(annotated,cv2.COLOR_BGR2RGB)) #OpenCV uses BGR format\n",
    "\n",
    "    display.c1ear_output(wait=True)\n",
    "    display.display(gcf())\n",
    "\n",
    "#When everything is done, release the capture\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
